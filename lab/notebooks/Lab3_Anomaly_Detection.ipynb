{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d244321",
      "metadata": {
        "id": "4d244321"
      },
      "source": [
        "# Laboratory 3 — Anomaly Detection\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NXfnBkl-P0ma",
      "metadata": {
        "id": "NXfnBkl-P0ma"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "svE8FspeRPW3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svE8FspeRPW3",
        "outputId": "16a0b897-7f01-45b3-ce24-6022253891fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.12.12\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n"
          ]
        }
      ],
      "source": [
        "# --- Check Python and pip versions ---\n",
        "!python --version\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fyfyzIzWbHTG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyfyzIzWbHTG",
        "outputId": "1557c8c2-d666-43e5-e280-2a9397bf451e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.11)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# --- Install required libraries ---\n",
        "!pip install torch\n",
        "!pip install numpy pandas scikit-learn matplotlib seaborn\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "vZXgjER_TsGp",
      "metadata": {
        "id": "vZXgjER_TsGp"
      },
      "outputs": [],
      "source": [
        "# --- Import libraries ---\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, log_loss\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w201oyWVxE9k",
      "metadata": {
        "id": "w201oyWVxE9k"
      },
      "source": [
        "### Colab Pro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "_kIggYNUxFuZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kIggYNUxFuZ",
        "outputId": "5515a424-8c51-4c29-8c2d-9a4d530f5b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "# --- Check GPU availability ---\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DH-w_NELxHpV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH-w_NELxHpV",
        "outputId": "4fb33a13-6bdb-4258-fc93-730e4ef1ddd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 8.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "# --- Check RAM availability ---\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2rFNp6fgTv9w",
      "metadata": {
        "id": "2rFNp6fgTv9w"
      },
      "source": [
        "### Paths setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "rAnAndzpTxe5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAnAndzpTxe5",
        "outputId": "273d2be7-f312-4f7a-a209-72aa66ea813e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# --- Mount Google Drive (for Google Colab users) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XqMS_38yTvFN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqMS_38yTvFN",
        "outputId": "b597e3a5-59eb-4531-d514-7e2c3417eae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project path: /content/drive/MyDrive/Projects/AImSecure/Laboratory2/\n",
            "Data path: /content/drive/MyDrive/Projects/AImSecure/Laboratory2/data/\n",
            "Results path: /content/drive/MyDrive/Projects/AImSecure/Laboratory2/results/\n"
          ]
        }
      ],
      "source": [
        "# --- Define Paths ---\n",
        "group = 'AImSecure'\n",
        "laboratory = 'Laboratory3'\n",
        "\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "project_path = base_path + f'Projects/{group}/{laboratory}/'\n",
        "data_path = project_path + 'data/'\n",
        "results_path = project_path + 'results/'\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "\n",
        "print(f\"Project path: {project_path}\")\n",
        "print(f\"Data path: {data_path}\")\n",
        "print(f\"Results path: {results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "pi-G9FFHwZjK",
      "metadata": {
        "id": "pi-G9FFHwZjK"
      },
      "outputs": [],
      "source": [
        "# --- Set visual style ---\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
        "\n",
        "def save_plot(fig: plt.Figure, filename: str, path: str = \"./plots/\", fmt: str = \"png\", dpi: int = 300, close_fig: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Save a Matplotlib figure in a specific to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        fig (plt.Figure): Matplotlib figure object to save.\n",
        "        filename (str): Name of the file to save (e.g., 'plot.png').\n",
        "        path (str, optional): Directory path to save the figure. Defaults to './plots/'.\n",
        "        fmt (str, optional): File format for the saved figure. Defaults to 'png'.\n",
        "        dpi (int, optional): Dots per inch for the saved figure. Defaults to 300.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    save_path = os.path.join(path, f\"{filename}.{fmt}\")\n",
        "\n",
        "    # Save the figure\n",
        "    fig.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=dpi, format=fmt)\n",
        "    # plt.close(fig) # Removed to display plots in notebook\n",
        "\n",
        "    if close_fig:\n",
        "        plt.close(fig)\n",
        "\n",
        "    print(f\"Saved plot: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dswv2Je3UA5Y",
      "metadata": {
        "id": "dswv2Je3UA5Y"
      },
      "source": [
        "## Task 1 — Dataset Characterization and Preprocessing\n",
        "\n",
        "text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "UbIMFRXuZxMB",
      "metadata": {
        "id": "UbIMFRXuZxMB"
      },
      "outputs": [],
      "source": [
        "# Create directory for plots\n",
        "save_dir = results_path + 'images/' + 'task1_plots/'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfca7af",
      "metadata": {},
      "source": [
        "### Explore the dataset\n",
        "\n",
        "Before preprocessing, we explore the data to understand the available features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1755aaaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# Code to explore the dataset\n",
        "# e.g., train_df.info()\n",
        "# e.g., train_df.describe()\n",
        "# Identify categorical and numerical features\n",
        "# Check distribution of 'attack_label' and 'binary_label'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b0890a9",
      "metadata": {},
      "source": [
        "#### Q: What are the dataset characteristics? How many categorical and numerical attributes do you have? How are your attack labels and binary label distributed?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2af34d8",
      "metadata": {},
      "source": [
        "### Preprocessing\n",
        "\n",
        "Preprocess features before performing any AI/ML algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3721fa4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# Implement preprocessing pipeline\n",
        "# e.g., Use StandardScaler for numerical features\n",
        "# e.g., Use OneHotEncoder for categorical features\n",
        "# Fit the preprocessors on train.csv and transform both train.csv and test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7217e89e",
      "metadata": {},
      "source": [
        "#### Q: How do you preprocess categorical and numerical data?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8623df0b",
      "metadata": {},
      "source": [
        "### Study your data from a domain expert perspective\n",
        "\n",
        "We will plot heatmaps that describe the statistical characteristics of each feature for each attack label. We consider 0/1 features as numerical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aece86b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Mean heatmap\n",
        "# Group data by 'attack_label' and calculate the mean of each feature\n",
        "# Plot the heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e64684",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 2. Standard Deviation heatmap\n",
        "# Group data by 'attack_label' and calculate the standard deviation\n",
        "# Plot the heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6caf9a43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 3. Median Heatmap\n",
        "# Group data by 'attack_label' and calculate the median\n",
        "# Plot the heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cccd765d",
      "metadata": {},
      "source": [
        "#### Q: Looking at the different heatmaps, do you find any main characteristics that are strongly correlated with a specific attack?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7SH8cyZ1K_",
      "metadata": {
        "id": "ad7SH8cyZ1K_"
      },
      "source": [
        "## Task 2 - Shallow Anomaly Detection - Supervised vs Unsupervised\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "QRKP-a8gZ1bH",
      "metadata": {
        "id": "QRKP-a8gZ1bH"
      },
      "outputs": [],
      "source": [
        "# Create directory for plots\n",
        "save_dir = results_path + 'images/' + 'task2_plots/'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f935b48",
      "metadata": {},
      "source": [
        "### One-Class SVM with Normal data only\n",
        "\n",
        "First, train a One-Class Support Vector Machine (OC-SVM) with benign (normal) traffic only using an rbf kernel. Then, evaluate the performance using all training data (normal + anomalies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168c60d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Filter the preprocessed training data to get 'normal' samples only\n",
        "# 2. Define two OC-SVM models (rbf kernel):\n",
        "#    - Model 1: Use your estimated 'nu'\n",
        "#    - Model 2: Use the default 'nu'\n",
        "# 3. Train both models on normal data only\n",
        "# 4. Evaluate both models on the *full* training set (normal + anomalies)\n",
        "# 5. Report classification report (precision, recall, f1-score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668a8514",
      "metadata": {},
      "source": [
        "#### Q: Considering that you are currently training only on normal data, which is a good estimate for the parameter $nu^{2}$? What is the impact on training performance? Try both your estimate and the default value of nu.\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfd086a",
      "metadata": {},
      "source": [
        "### One-Class SVM with All data\n",
        "\n",
        "Now train the OC-SVM with both normal and anomalous data. Estimate nu as the ratio of anomalous data across the entire collection. Then, evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9025030",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Calculate 'nu' as the ratio of anomalies in the full training set\n",
        "# 2. Train a new OC-SVM (rbf kernel) on the *full* training set using this 'nu'\n",
        "# 3. Evaluate the model on the full training set\n",
        "# 4. Report classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2fc0354",
      "metadata": {},
      "source": [
        "#### Q: Which model performs better? Why do you think that?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108071f6",
      "metadata": {},
      "source": [
        "### One-Class SVM with normal traffic and some anomalies\n",
        "\n",
        "Evaluate the impact of the percentage of anomalies while training. Train several OC-SVMs with an increasing subsample of anomalous classes (10%, 20%, 50%, 100% of anomalies). Estimate the nu parameter for each scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5218b19d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Get all normal data\n",
        "# 2. Get all anomalous data\n",
        "# 3. Create a loop for percentages [0.1, 0.2, 0.5, 1.0]\n",
        "# 4. In the loop:\n",
        "#    - Subsample the anomalous data based on the percentage\n",
        "#    - Combine with all normal data to create a new training subset\n",
        "#    - Calculate 'nu' for this subset (ratio of anomalies)\n",
        "#    - Train an OC-SVM on this subset\n",
        "#    - Evaluate the trained model on the *full* training set (normal + all anomalies)\n",
        "#    - Store the f1-macro score\n",
        "# 5. Plot the f1-macro scores against the anomaly percentages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51250fda",
      "metadata": {},
      "source": [
        "#### Q: Plot the f1-macro score for each scenario. How does the increasing ratio of anomalies affect the results?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dfabeb6",
      "metadata": {},
      "source": [
        "### Robustness of the One-Class SVM model\n",
        "\n",
        "Finally, use the test set to assess the robustness. [cite: 106] Use models trained with:\n",
        "1. Only normal data\n",
        "2. All data\n",
        "3. 10% of anomalous data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d443dbbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Take the three models trained previously (from point 1, 2, and 3 of Task 2)\n",
        "# 2. Evaluate each of them on the *preprocessed test set*\n",
        "# 3. For each model, print the classification report and a confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a16e57",
      "metadata": {},
      "source": [
        "#### Q: Is the best-performing model in the training set also the best here? Does it confuse normal data with anomalies? Which attack is the most confused?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4tuRqX7NZ1vJ",
      "metadata": {
        "id": "4tuRqX7NZ1vJ"
      },
      "source": [
        "## Task 3 - Deep Anomaly Detection and Data Representation\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "GBCKFczDZ2Fz",
      "metadata": {
        "id": "GBCKFczDZ2Fz"
      },
      "outputs": [],
      "source": [
        "# Create directory for plots\n",
        "save_dir = results_path + 'images/' + 'task3_plots/'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f96eac",
      "metadata": {},
      "source": [
        "### Training and Validating Autoencoder with Normal data only\n",
        "\n",
        "Create an Autoencoder with a shrinking encoder and an expansion decoder. Use normal data only; split into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87282cb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Get the preprocessed 'normal' data from the training set\n",
        "# 2. Split this normal data into a new AE-training set and an AE-validation set (e.g., 80/20 split)\n",
        "# 3. Define the Autoencoder (AE) architecture using Keras/TensorFlow\n",
        "#    - Shrinking encoder\n",
        "#    - Bottleneck layer\n",
        "#    - Expansion decoder\n",
        "# 4. Compile the model (e.g., optimizer='adam', loss='mse')\n",
        "# 5. Train the model using the AE-training set and validate on the AE-validation set\n",
        "# 6. Use callbacks like EarlyStopping to find the best number of epochs\n",
        "# 7. Plot the training and validation loss curves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae419dd5",
      "metadata": {},
      "source": [
        "### Estimate the Reconstruction Error Threshold\n",
        "\n",
        "Estimate a threshold using the reconstruction error on the validation set. Plot the ECDF curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeea97a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Get predictions (reconstructions) for the AE-validation set\n",
        "# 2. Calculate the reconstruction error (e.g., MSE) for each sample in the validation set\n",
        "# 3. Plot the ECDF (Empirical Cumulative Distribution Function) of these errors\n",
        "# 4. Choose a threshold based on the ECDF (e.g., 95th or 99th percentile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "139fefa4",
      "metadata": {},
      "source": [
        "#### Q: How did you pick the threshold? What is its value?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec57b8a",
      "metadata": {},
      "source": [
        "### Anomaly Detection with reconstruction error\n",
        "\n",
        "Use the trained model and threshold to classify anomalies in the full training set and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a17e49c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Calculate reconstruction errors for:\n",
        "#    - i) AE-validation set (already done)\n",
        "#    - ii) Full training set (normal + anomalies)\n",
        "#    - iii) Full test set\n",
        "# 2. Plot the ECDFs for all three sets of errors on one graph\n",
        "# 3. Using the threshold from the previous step:\n",
        "#    - Classify anomalies in the full training set\n",
        "#    - Classify anomalies in the test set\n",
        "# 4. Report classification reports for both"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e199a57",
      "metadata": {},
      "source": [
        "#### Q: Plot and report the ECDF... Why the reconstruction errors higher on the full training set than on the validation one? And why are the reconstruction errors in the test set even higher? How is the performance on the training... and test set?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d061d27",
      "metadata": {},
      "source": [
        "### Auto-Encoder's bottleneck and OC-SVM\n",
        "\n",
        "Use the encoder's bottleneck for data representation. Train an OC-SVM on the bottleneck embeddings of the normal data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffa8942",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Create an 'encoder' model from the trained AE (inputs -> bottleneck layer)\n",
        "# 2. Extract bottleneck embeddings for the *normal* training data\n",
        "# 3. Train a new OC-SVM on these normal data embeddings (like in Task 2.1)\n",
        "# 4. Extract bottleneck embeddings for the *full test set*\n",
        "# 5. Use the trained OC-SVM to predict anomalies on the test embeddings\n",
        "# 6. Report the classification report for the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c980ccf0",
      "metadata": {},
      "source": [
        "#### Q: Compare the results with the best original OC-SVM and with the Autoencoder with reconstruction error. Describe the performance...\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d5eadd",
      "metadata": {},
      "source": [
        "### PCA and OC-SVM\n",
        "\n",
        "Use PCA for data representation. Analyze the explained variance on normal data only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c14455",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Use the preprocessed *normal* training data\n",
        "# 2. Fit PCA on this data (try a large number of components first, e.g., n_components=0.99)\n",
        "# 3. Plot the cumulative explained variance vs. number of components\n",
        "# 4. Identify the 'elbow point' or the number of components explaining (e.g.) 95% of the variance\n",
        "# 5. Fit and transform the normal training data with this 'best' number of components\n",
        "# 6. Transform the *full test set* with the *same fitted PCA*\n",
        "# 7. Train an OC-SVM on the PCA-transformed normal training data\n",
        "# 8. Evaluate this OC-SVM on the PCA-transformed test set\n",
        "# 9. Report classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0ef987",
      "metadata": {},
      "source": [
        "#### Q: compare results with the original OC-SVM and the OC-SVM trained using the Encoder embeddings. Describe the performance... \n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "McIjNvYYZ2S1",
      "metadata": {
        "id": "McIjNvYYZ2S1"
      },
      "source": [
        "## Task 4 - Unsupervised Anomaly Detection and Interpretation\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "Cf1bbg_eZ2rk",
      "metadata": {
        "id": "Cf1bbg_eZ2rk"
      },
      "outputs": [],
      "source": [
        "# Create directory for plots\n",
        "save_dir = results_path + 'images/' + 'task4_plots/'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f11090",
      "metadata": {},
      "source": [
        "### K-means with little domain knowledge\n",
        "\n",
        "Fit k-means with 4 clusters and the full training data (normal + anomalous). (Normal + 3 attack types = 4 clusters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee04c17b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Use the *full preprocessed training data*\n",
        "# 2. Fit a KMeans model with n_clusters=4\n",
        "# 3. Get the cluster assignments (labels) for all training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c49805",
      "metadata": {},
      "source": [
        "### K-means cluster interpretation\n",
        "\n",
        "Examine the clusters to understand their quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73fc04be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Add the K-means cluster labels to the training dataframe (which has the real attack labels)\n",
        "# 2. Calculate and print the size of each cluster\n",
        "# 3. Create a contingency table (crosstab) of K-means clusters vs. true 'attack_label'\n",
        "# 4. Calculate silhouette scores per cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa91dfb4",
      "metadata": {},
      "source": [
        "#### Q: How big are the clusters? How are the attack labels distributed across the clusters? Are the clusters pure?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed857889",
      "metadata": {},
      "source": [
        "#### Q: How high is the silhouette per cluster? Is there any clusters with a lower silhouette value? If it is the case, what attack labels are present in these clusters?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c39e06e",
      "metadata": {},
      "source": [
        "#### Q: Use the t-SNE algorithm... Plot and report: i) t-SNE... with cluster ID. ii) t-SNE... with the attack label. \n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa17e4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Run t-SNE on the full training data (n_components=2)\n",
        "#    - Try a few perplexity values (e.g., 30, 50, 100)\n",
        "# 2. Choose the best-looking perplexity\n",
        "# 3. Plot 1: 2D t-SNE scatter plot, colored by *K-means cluster ID*\n",
        "# 4. Plot 2: 2D t-SNE scatter plot, colored by *true attack_label*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4512014",
      "metadata": {},
      "source": [
        "#### Q: Can you find a difference between the two visualizations? What are the misinterpreted points?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de407d6",
      "metadata": {},
      "source": [
        "### DB-Scan anomalies are anomalies?\n",
        "\n",
        "Use DB-Scan to detect anomalous patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6910941",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CODE BLOCK]\n",
        "# 1. Determine 'min_points'\n",
        "#    - (Justify your choice. e.g., \"From the k-means analysis, the smallest 'pure' normal cluster was not found. We will set min_points=... based on domain knowledge/heuristics...\") \n",
        "# 2. Determine 'eps' (e) using the elbow rule \n",
        "#    - Calculate distance to the k-th nearest neighbor (k=min_points)\n",
        "#    - Sort and plot these distances\n",
        "#    - Find the 'elbow' (point of max curvature)\n",
        "# 3. Run DBSCAN on the full training set with your chosen 'eps' and 'min_points' \n",
        "# 4. Get the DBSCAN cluster labels (includes -1 for noise)\n",
        "# 5. Check the noise cluster (label -1)\n",
        "# 6. Create a crosstab of DBSCAN cluster labels vs. true 'binary_label' (0=normal, 1=anomaly)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c9077df",
      "metadata": {},
      "source": [
        "#### Q: Create the clustering results... Does the DB-Scan noise cluster (cluster -1) consist only of anomalous points (cross-reference with real attack labels)?\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wm7xGQVmJBZs",
      "metadata": {
        "id": "wm7xGQVmJBZs"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
